{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b5e63a-16f7-448d-8743-1c5252d5a4a8",
   "metadata": {},
   "source": [
    "#### Importações e Funções de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e4f79a-b591-472c-9e47-4e58fd91671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import tomotopy as tp\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "# emojis and punctuation\n",
    "punct = list(string.punctuation) + [\"\\n\"]\n",
    "keep_punct = [\"-\"]\n",
    "[punct.remove(item) for item in keep_punct if item in punct]\n",
    "\n",
    "emojis_list = list(emoji.EMOJI_DATA.keys())\n",
    "emojis_list += [\"\\n\"]\n",
    "emojis_punct = emojis_list + punct\n",
    "\n",
    "def processEmojisPunctuation(text, remove_punct=False, remove_emoji=False):\n",
    "    \"\"\"\n",
    "    Put spaces between emojis. Removes punctuation.\n",
    "    \"\"\"\n",
    "    # get all unique chars\n",
    "    chars = set(text)\n",
    "    # for each unique char in text, do:\n",
    "    for c in chars:\n",
    "        if remove_punct:  # remove punctuation\n",
    "            if c in punct:\n",
    "                text = text.replace(c, \" \")\n",
    "\n",
    "        if remove_emoji:  # remove emojis\n",
    "            if c in emojis_list:\n",
    "                text = text.replace(c, \" \")\n",
    "        else:  # put spaces between emojis\n",
    "            if c in emojis_list:\n",
    "                text = text.replace(c, \" \" + c + \" \")\n",
    "\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# stop words removal\n",
    "stop_words = list(stopwords.words(\"portuguese\"))\n",
    "new_stopwords = [\n",
    "    \"aí\",\n",
    "    \"pra\",\n",
    "    \"vão\",\n",
    "    \"vou\",\n",
    "    \"onde\",\n",
    "    \"tá\",\n",
    "    \"pois\",\n",
    "    \"so\",\n",
    "    \"deu\",\n",
    "    \"ai\",\n",
    "    \"ta\",\n",
    "    \"alguem\",\n",
    "    \"ne\",\n",
    "    \"cara\",\n",
    "    \"to\",\n",
    "    \"mim\",\n",
    "    \"la\",\n",
    "    \"vcs\",\n",
    "    \"tbm\",\n",
    "    \"tudo\",\n",
    "    \"a\",\n",
    "    \"O\",\n",
    "    \"uma\",\n",
    "    \"de\",\n",
    "    \"que\"\n",
    "]\n",
    "stop_words = stop_words + new_stopwords\n",
    "final_stop_words = []\n",
    "for sw in stop_words:\n",
    "    sw = \" \" + sw + \" \"\n",
    "    final_stop_words.append(sw)\n",
    "\n",
    "def removeStopwords(text):\n",
    "    for sw in final_stop_words:\n",
    "        text = text.replace(sw, \" \")\n",
    "    return text\n",
    "\n",
    "# lemmatization\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text != token.lemma_:\n",
    "            text = text.replace(token.text, token.lemma_)\n",
    "    return text\n",
    "\n",
    "def domainUrl(text):\n",
    "    \"\"\"\n",
    "    Substitutes an URL in a text for the domain of this URL\n",
    "    Input: an string\n",
    "    Output: the string with the modified URL\n",
    "    \"\"\"\n",
    "    if \"http\" in text:\n",
    "        re_url = r\"[^\\s]*https*://[^\\s]*\"\n",
    "        matches = re.findall(re_url, text, flags=re.IGNORECASE)\n",
    "        for m in matches:\n",
    "            domain = m.split(\"//\")\n",
    "            domain = domain[1].split(\"/\")[0]\n",
    "            text = re.sub(re_url, domain, text, 1)\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def processLoL(text):\n",
    "    re_kkk = r\"kkk*\"\n",
    "    t = re.sub(re_kkk, \"kkk\", text, flags=re.IGNORECASE)\n",
    "    return t\n",
    "\n",
    "def preprocess(text, semi=False, rpunct=False, remoji=False):\n",
    "    text = text.lower().strip()\n",
    "    text = domainUrl(text)\n",
    "    text = processLoL(text)\n",
    "    text = processEmojisPunctuation(text, remove_punct=rpunct, remove_emoji=remoji)\n",
    "    if semi:\n",
    "        return text\n",
    "    text = removeStopwords(text)\n",
    "    text = lemmatization(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540048f3-0f6c-4fba-9984-f645497ee82f",
   "metadata": {},
   "source": [
    "#### Carregar e Pré-processar o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485d0b25-2819-4b57-bafc-570f2ad39ede",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../transcricoes_medium/m_transc_Bernardo_P_Küster.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Carregar o dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../transcricoes_medium/m_transc_Bernardo_P_Küster.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Limpeza e preprocessamento\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[:\u001b[38;5;241m3000\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../transcricoes_medium/m_transc_Bernardo_P_Küster.csv'"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset\n",
    "dataset = pd.read_csv('../transcricoes_medium/m_transc_Bernardo_P_Küster.csv')\n",
    "\n",
    "# Limpeza e preprocessamento\n",
    "dataset = dataset[:3000]\n",
    "dataset['Transcription'] = dataset['Transcription'].apply(lambda x: preprocess(x, semi=True, rpunct=True, remoji=True))\n",
    "dataset.dropna(subset=['Transcription'], inplace=True)\n",
    "clean_messages = dataset['Transcription'].apply(lambda x: preprocess(x, semi=True, rpunct=True, remoji=True))\n",
    "\n",
    "# Tokenização e preparação dos textos\n",
    "token_messages = [nlp(x) for x in clean_messages]\n",
    "texts = [[token.text for token in doc] for doc in token_messages]\n",
    "\n",
    "# Criar o corpus para tomotopy\n",
    "corpus = tp.utils.Corpus()\n",
    "for text in texts:\n",
    "    corpus.add_doc(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1c29c-6a0c-4855-8f94-0cd0ec64b862",
   "metadata": {},
   "source": [
    "#### Definir Funções para Diversidade de Tópicos, iRBO e Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aca20d-de4e-44b3-bcd2-38c68f22fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_unique_words(topics, topk=10):\n",
    "    \"\"\"\n",
    "    Calcular a proporção de palavras únicas nos tópicos\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: lista de listas de palavras\n",
    "    topk: top k palavras nas quais a diversidade do tópico será calculada\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than '+str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        puw = len(unique_words) / (topk * len(topics))\n",
    "        return puw\n",
    "\n",
    "def calculate_irbo(pt_model, topk=10):\n",
    "    \"\"\"\n",
    "    Calcular o iRBO entre tópicos\n",
    "    \"\"\"\n",
    "    import irbo\n",
    "    topics = [pt_model.get_topic_words(k, top_n=topk) for k in range(pt_model.k)]\n",
    "    topics = [[word for word, _ in topic] for topic in topics]\n",
    "    irbo_values = []\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            irbo_value = irbo.rank_biased_overlap(topics[i], topics[j], p=0.9)\n",
    "            irbo_values.append(irbo_value)\n",
    "    return np.mean(irbo_values)\n",
    "\n",
    "def calculate_jaccard_distance(pt_model, topk=10):\n",
    "    \"\"\"\n",
    "    Calcular a distância de Jaccard entre tópicos\n",
    "    \"\"\"\n",
    "    def jaccard_distance(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return 1 - intersection / union\n",
    "\n",
    "    topics = [pt_model.get_topic_words(k, top_n=topk) for k in range(pt_model.k)]\n",
    "    topics = [set([word for word, _ in topic]) for topic in topics]\n",
    "    jaccard_values = []\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            jaccard_value = jaccard_distance(topics[i], topics[j])\n",
    "            jaccard_values.append(jaccard_value)\n",
    "    return np.mean(jaccard_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812bbc1-6625-43b8-b0d6-112d14082eb5",
   "metadata": {},
   "source": [
    "#### Treinar Modelos e Calcular Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb003f95-dd2d-4ba7-b26f-e186932cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a faixa de tópicos\n",
    "start = 2\n",
    "limit = 30\n",
    "step = 2\n",
    "\n",
    "metric_values = []\n",
    "\n",
    "# Treinamento dos modelos de tópicos e cálculo das métricas\n",
    "for num_topics in tqdm(range(start, limit, step)):\n",
    "    pt_model = tp.PTModel(p=1000, k=num_topics, seed=42)\n",
    "    for doc in corpus:\n",
    "        pt_model.add_doc(doc.words)\n",
    "    pt_model.train(100)  # Esse 100 vem de um hiperparametro, pode mudar depois\n",
    "\n",
    "    metric_value_row = {'num_topics': num_topics}  # Armazena o número de tópicos\n",
    "\n",
    "    # Calcular as métricas de coerência\n",
    "    for preset in ('u_mass', 'c_uci', 'c_npmi', 'c_v'):\n",
    "        coh = tp.coherence.Coherence(pt_model, coherence=preset)\n",
    "        average_coherence = coh.get_score()\n",
    "        metric_value_row[preset] = average_coherence\n",
    "\n",
    "    # Calcular a perplexidade\n",
    "    metric_value_row['perplexity'] = pt_model.perplexity\n",
    "\n",
    "    # Calcular a diversidade dos tópicos\n",
    "    topics = [pt_model.get_topic_words(k, top_n=10) for k in range(num_topics)]\n",
    "    topics = [[word for word, _ in topic] for topic in topics]\n",
    "    metric_value_row['diversity'] = proportion_unique_words(topics, topk=10)\n",
    "\n",
    "    # Calcular o iRBO entre os tópicos\n",
    "    metric_value_row['irbo'] = calculate_irbo(pt_model, topk=10)\n",
    "\n",
    "    # Calcular a distância de Jaccard entre os tópicos\n",
    "    metric_value_row['jaccard'] = calculate_jaccard_distance(pt_model, topk=10)\n",
    "\n",
    "    metric_values.append(metric_value_row)\n",
    "\n",
    "# Salvar as métricas em um CSV\n",
    "metrics_df = pd.DataFrame(metric_values)\n",
    "metrics_df.to_csv('topic_model_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05963519-d736-4233-8149-eccc2e3a85fb",
   "metadata": {},
   "source": [
    "#### Visualizar as Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5132725-e3fd-4807-a767-4892c8ce3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a coerência dos tópicos\n",
    "coherence_values = [x['c_v'] for x in metric_values]\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Score de Coerência\")\n",
    "plt.legend((\"Valores de Coerência\"), loc='best')\n",
    "plt.savefig(\"coherence_score.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plotar a diversidade dos tópicos\n",
    "diversity_values = [x['diversity'] for x in metric_values]\n",
    "plt.plot(x, diversity_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Diversidade dos Tópicos\")\n",
    "plt.legend((\"Diversidade dos Tópicos\"), loc='best')\n",
    "plt.savefig(\"diversity_score.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plotar o iRBO entre os tópicos\n",
    "irbo_values = [x['irbo'] for x in metric_values]\n",
    "plt.plot(x, irbo_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"iRBO\")\n",
    "plt.legend((\"iRBO\"), loc='best')\n",
    "plt.savefig(\"irbo_score.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plotar a distância de Jaccard entre os tópicos\n",
    "jaccard_values = [x['jaccard'] for x in metric_values]\n",
    "plt.plot(x, jaccard_values)\n",
    "plt.xlabel(\"Num Tópicos\")\n",
    "plt.ylabel(\"Distância de Jaccard\")\n",
    "plt.legend((\"Distância de Jaccard\"), loc='best')\n",
    "plt.savefig(\"jaccard_score.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902014d7-6f4a-4b83-9306-c42841b6d72a",
   "metadata": {},
   "source": [
    "#### Salvar e Visualizar Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7118386-cd6d-4ca2-86c2-5b8cc8a74e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os tópicos em um CSV e imprimir os tópicos\n",
    "def save_and_print_topics(pt_model, num_topics, num_words, filename):\n",
    "    topics = []\n",
    "    for k in range(num_topics):\n",
    "        topic_words = pt_model.get_topic_words(k, top_n=num_words)\n",
    "        topics.append([word for word, prob in topic_words])\n",
    "        print(f'Tópico {k}: {[word for word, prob in topic_words]}')\n",
    "\n",
    "    topics_df = pd.DataFrame(topics)\n",
    "    topics_df.insert(0, 'Tópico', range(num_topics))\n",
    "    topics_df.to_csv(filename, index=False)\n",
    "\n",
    "save_and_print_topics(pt_model, num_topics=10, num_words=30, filename='bernardo_topicos_tomotopy.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b85ef2-bca9-4c67-98d7-1e4015e5b039",
   "metadata": {},
   "source": [
    "#### Nuvem de Palavras e Visualização Interativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52340a-bc32-4c2d-a223-63d752bc1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuvem de Palavras Única\n",
    "def plot_combined_word_cloud(model, num_topics, filename):\n",
    "    combined_dict = {}\n",
    "    for t in range(num_topics):\n",
    "        topic_words = model.get_topic_words(t, top_n=30)\n",
    "        for word, weight in topic_words:\n",
    "            if word in combined_dict:\n",
    "                combined_dict[word] += weight\n",
    "            else:\n",
    "                combined_dict[word] = weight\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(combined_dict))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Nuvem de Palavras Combinada\")\n",
    "    plt.savefig(filename)  # Salvar a nuvem de palavras\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_word_cloud(pt_model, num_topics=pt_model.k, filename='word_cloud.png')\n",
    "\n",
    "# Preparar os dados para visualização com pyLDAvis\n",
    "topic_term_dists = np.stack([pt_model.get_topic_word_dist(k) for k in range(pt_model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in pt_model.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in pt_model.docs])\n",
    "vocab = list(pt_model.used_vocabs)\n",
    "term_frequency = pt_model.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists=topic_term_dists,\n",
    "    doc_topic_dists=doc_topic_dists,\n",
    "    doc_lengths=doc_lengths,\n",
    "    vocab=vocab,\n",
    "    term_frequency=term_frequency\n",
    ")\n",
    "\n",
    "# Salvar a visualização em um arquivo HTML\n",
    "pyLDAvis.save_html(prepared_data, 'lda_vis.html')\n",
    "pyLDAvis.show(prepared_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
